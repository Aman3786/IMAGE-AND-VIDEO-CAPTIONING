# IMAGE-AND-VIDEO-CAPTIONING
Created an image &amp; video captioning Deep learning model for blind people to make it easy to understand the scenario infront of them.

# FEATURES
  - Model trained on Google Colab.
  - Ability to predict the suitable captions by taking the input as an image or video.
  
# TECHNOLOGY USED 
  - Google Colab 
  - Python 
  - Tensorflow and keras 
  - Transfer learning ( Inception v3 - Pretained Models)
  - Opencv 
  
# FUTURE IMPROVEMENT 
  - We can Implement Text summarizer concept to summarise the content that we will get from the Output of captioning model.
  - After Summarising the content, we can output the text in the form of TEXT-TO-SPEECH. 
  - It will make more efficient for blind people to understand the scenario.
